{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b92041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from numpy.typing import NDArray\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582a5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KArmedGaussianBandit():\n",
    "    def __init__(self, k, mu : NDArray | None = None, std : NDArray | None = None):\n",
    "        self.k = k\n",
    "        self.mu = mu if mu is not None else np.zeros(k)\n",
    "        self.std = std if std is not None else np.ones(k)\n",
    "\n",
    "    def pull(self, arm):\n",
    "        return np.random.normal(self.mu[arm], self.std[arm])\n",
    "\n",
    "class BanditAgent():\n",
    "    def __init__(\n",
    "            self,\n",
    "            bandit : KArmedGaussianBandit,\n",
    "            q_initial : NDArray | None = None,\n",
    "            strategy : str = \"epsilon-greedy\",\n",
    "            epsilon : float = 0.1\n",
    "        ):\n",
    "        self.bandit = bandit\n",
    "        self.n = np.zeros(bandit.k)\n",
    "        self.strategy = strategy\n",
    "        self.epsilon = epsilon\n",
    "        self.total_reward = 0.0\n",
    "        if q_initial is not None:\n",
    "            if len(q_initial) == bandit.k:\n",
    "                self.q_estimates = q_initial\n",
    "            else:\n",
    "                raise ValueError(f\"Expected q_initial of length {bandit.k}, got {len(q_initial)}\")\n",
    "        else:\n",
    "            self.q_estimates = np.zeros(bandit.k)\n",
    "\n",
    "    def play_episode(self):\n",
    "        k = self.select_arm()\n",
    "        reward = self.bandit.pull(k)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        self.update_estimates(k, reward)\n",
    "\n",
    "    def select_arm(self):\n",
    "        if self.strategy == \"epsilon-greedy\":\n",
    "            return self._select_arm_epsilon_greedy()\n",
    "        elif self.strategy == \"ucb\":\n",
    "            return self._select_arm_ucb()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "\n",
    "    def _select_arm_epsilon_greedy(self):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.bandit.k)\n",
    "        else:\n",
    "            return np.argmax(self.q_estimates).item()\n",
    "\n",
    "    def _select_arm_ucb(self):\n",
    "        total_counts = self.n.sum()\n",
    "        if total_counts == 0:\n",
    "            return np.random.choice(self.bandit.k)\n",
    "        ucb_values = self.q_estimates + np.sqrt(2 * np.log(total_counts) / (self.n + 1e-5))\n",
    "        return np.argmax(ucb_values).item()\n",
    "\n",
    "    def update_estimates(self, k, reward):\n",
    "        self.q_estimates[k] += (reward - self.q_estimates[k]) / (self.n[k] + 1)\n",
    "        self.n[k] += 1\n",
    "\n",
    "    def get_average_reward(self):\n",
    "        return self.total_reward / self.n.sum()\n",
    "\n",
    "    def print_estimates(self):\n",
    "        print(\"Estimated action values:\")\n",
    "        for i in range(self.bandit.k):\n",
    "            print(f\"Arm {i}: {self.q_estimates[i]:.2f} (n={self.n[i]})\")\n",
    "\n",
    "\n",
    "class GradientBanditAgent():\n",
    "    def __init__(\n",
    "            self,\n",
    "            bandit : KArmedGaussianBandit,\n",
    "            alpha : float = 0.1\n",
    "        ):\n",
    "        self.bandit = bandit\n",
    "        self.n = 0\n",
    "        self.alpha = alpha\n",
    "        self.average_reward = 0.0\n",
    "\n",
    "        self.preferences = np.zeros(bandit.k)\n",
    "\n",
    "    def play_episode(self):\n",
    "        k = self.select_arm()\n",
    "        reward = self.bandit.pull(k)\n",
    "        self.average_reward += (reward - self.average_reward) / (self.n + 1)\n",
    "\n",
    "        self.update_preferences(k, reward)\n",
    "\n",
    "    def select_arm(self):\n",
    "        probs = softmax(self.preferences)\n",
    "        return np.random.choice(self.bandit.k, p=probs)\n",
    "\n",
    "    def update_preferences(self, k, reward):\n",
    "        kronecker_delta = np.zeros(self.bandit.k)\n",
    "        kronecker_delta[k] = 1.0\n",
    "        self.preferences = (\n",
    "            self.preferences + self.alpha * (reward - self.average_reward) \n",
    "            * (kronecker_delta - softmax(self.preferences))\n",
    "        )\n",
    "\n",
    "    def get_average_reward(self):\n",
    "        return self.average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7522ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "58e99b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated action values:\n",
      "Arm 0: 0.00 (n=0.0)\n",
      "Arm 1: 0.00 (n=0.0)\n",
      "Arm 2: 0.00 (n=0.0)\n",
      "Arm 3: 0.00 (n=0.0)\n",
      "Arm 4: 0.00 (n=0.0)\n",
      "Arm 5: 0.00 (n=0.0)\n",
      "Arm 6: 0.00 (n=0.0)\n",
      "Arm 7: 0.00 (n=0.0)\n",
      "Arm 8: 0.00 (n=0.0)\n",
      "Arm 9: 0.00 (n=0.0)\n"
     ]
    }
   ],
   "source": [
    "rewards = np.array([\n",
    "    1.0, 0.5, 0.2, 0.8, 0.3, 0.9, 0.4, 0.6, 0.7, 0.1\n",
    "])\n",
    "bandit = KArmedGaussianBandit(k=10, mu=rewards, std=np.ones(10))\n",
    "\n",
    "q_initial = np.ones_like(rewards) * 0.0  # Initial estimates for each arm\n",
    "\n",
    "agent = BanditAgent(bandit, q_initial=q_initial, strategy=\"epsilon-greedy\", epsilon=0.1)\n",
    "agent.print_estimates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fa5de21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated action values:\n",
      "Arm 0: 1.00 (n=8472.0)\n",
      "Arm 1: 0.61 (n=100.0)\n",
      "Arm 2: 0.30 (n=115.0)\n",
      "Arm 3: 0.69 (n=162.0)\n",
      "Arm 4: 0.22 (n=84.0)\n",
      "Arm 5: 0.89 (n=640.0)\n",
      "Arm 6: 0.33 (n=114.0)\n",
      "Arm 7: 0.66 (n=113.0)\n",
      "Arm 8: 0.75 (n=95.0)\n",
      "Arm 9: 0.03 (n=105.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    agent.play_episode()\n",
    "agent.print_estimates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dea10c2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9426117986341529)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.get_average_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b4793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
