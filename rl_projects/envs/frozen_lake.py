from typing import Any, Tuple, cast

import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.axes import Axes
from numpy.typing import NDArray


def get_default_env(
    render_mode: str = "ansi",
    is_slippery: bool = False,
    success_rate: float = 1.0 / 3.0,
    map_name: str = "4x4",
) -> Any:
    env: Any = cast(
        Any,
        gym.make(
            "FrozenLake-v1",
            desc=None,
            map_name=map_name,
            is_slippery=is_slippery,
            success_rate=success_rate,
            reward_schedule=(1, 0, 0),
            render_mode=render_mode,
        ),
    )

    return env


def random_deterministic_policy(env: Any) -> NDArray:
    """Generates a random deterministic policy for a given environment.

    Args:
        env: The Gym environment to generate the policy for. It should have
            observation_space and action_space attributes.

    Returns:
        np.ndarray: A one-hot encoded policy matrix of shape (obs_space.n, action_space.n).
    """
    obs_space = env.observation_space
    action_space = env.action_space
    policy = np.random.multinomial(
        1,
        [1 / action_space.n] * action_space.n,  # type: ignore
        size=obs_space.n,  # type: ignore
    )

    return policy


def random_stochastic_policy(env: Any) -> NDArray:
    """Generates a random stochastic policy for a given environment.

    The policy is generated by applying a softmax transformation to randomly sampled values.

    Args:
        env: The Gym environment to generate the policy for. It should have
            observation_space and action_space attributes.

    Returns:
        np.ndarray: A probability distribution matrix of shape (obs_space.n, action_space.n).
    """
    x = np.random.random((env.observation_space.n, env.action_space.n))  # type: ignore
    softmax_x = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)

    return softmax_x


# optimal policy for standard 4x4 non-slippery
OPTIMAL_FROZEN_LAKE_POLICY = np.array(
    [
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [1, 0, 0, 0],
    ]
)


def visualize_Q_function(
    Q: NDArray, grid_shape: Tuple[int, int] = (4, 4), ax: Axes | None = None
):
    rows = [[] for _ in range(grid_shape[0])]
    for i, actions in enumerate(Q):
        current_row = i % grid_shape[0]
        action_block = np.array([None] * 9).reshape(3, 3)
        action_block[0, 1] = actions[3]
        action_block[1, 0] = actions[0]
        action_block[1, 2] = actions[2]
        action_block[2, 1] = actions[1]
        action_block = action_block.astype(float)
        rows[current_row].append(action_block)

    board = np.concatenate([np.concatenate(row).T for row in rows])

    if ax is None:
        _, ax = plt.subplots(figsize=(10, 10))
    sns.heatmap(board.T, annot=True, cmap="viridis", cbar=False, ax=ax)

    return ax
