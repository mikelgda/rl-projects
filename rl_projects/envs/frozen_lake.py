from typing import Any, Tuple, cast

import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from matplotlib.axes import Axes
from numpy.typing import NDArray


def get_default_env(
    render_mode: str = "ansi",
    is_slippery: bool = False,
    success_rate: float = 1.0 / 3.0,
    map_name: str = "4x4",
) -> Any:
    env: Any = cast(
        Any,
        gym.make(
            "FrozenLake-v1",
            desc=None,
            map_name=map_name,
            is_slippery=is_slippery,
            success_rate=success_rate,
            reward_schedule=(1, 0, 0),
            render_mode=render_mode,
        ),
    )

    return env


def random_deterministic_policy(env: Any) -> NDArray:
    """Generates a random deterministic policy for a given environment.

    Args:
        env: The Gym environment to generate the policy for. It should have
            observation_space and action_space attributes.

    Returns:
        np.ndarray: A one-hot encoded policy matrix of shape (obs_space.n, action_space.n).
    """
    obs_space = env.observation_space
    action_space = env.action_space
    policy = np.random.multinomial(
        1,
        [1 / action_space.n] * action_space.n,  # type: ignore
        size=obs_space.n,  # type: ignore
    )

    return policy


def random_stochastic_policy(env: Any) -> NDArray:
    """Generates a random stochastic policy for a given environment.

    The policy is generated by applying a softmax transformation to randomly sampled values.

    Args:
        env: The Gym environment to generate the policy for. It should have
            observation_space and action_space attributes.

    Returns:
        np.ndarray: A probability distribution matrix of shape (obs_space.n, action_space.n).
    """
    x = np.random.random((env.observation_space.n, env.action_space.n))  # type: ignore
    softmax_x = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)

    return softmax_x


# optimal policy for standard 4x4 non-slippery
OPTIMAL_FROZEN_LAKE_POLICY = np.array(
    [
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 1, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [1, 0, 0, 0],
    ]
)


def visualize_Q_function(
    Q: NDArray, grid_shape: Tuple[int, int] = (4, 4), ax: Axes | None = None
):
    rows = [[] for _ in range(grid_shape[0])]
    for i, actions in enumerate(Q):
        current_row = i // grid_shape[1]
        action_block = np.array([None] * 9).reshape(3, 3)
        action_block[0, 1] = actions[3]
        action_block[1, 0] = actions[0]
        action_block[1, 2] = actions[2]
        action_block[2, 1] = actions[1]
        action_block = action_block.astype(float)
        rows[current_row].append(action_block)

    board = np.concatenate([np.concatenate(row, axis=1) for row in rows])

    if ax is None:
        _, ax = plt.subplots(figsize=(10, 10))
    sns.heatmap(board, annot=True, cmap="viridis", cbar=False, ax=ax)

    return ax


def visualize_Q_function_v2(
    Q: NDArray, grid_shape: Tuple[int, int] = (4, 4), ax: Axes | None = None
) -> Axes:
    """Visualize Q-function with arrows showing action values.

    Args:
        Q: Q-function array of shape (n_states, n_actions)
        grid_shape: Shape of the grid environment
        ax: Optional matplotlib axes to plot on

    Returns:
        The matplotlib axes object
    """

    if ax is None:
        fig, ax = plt.subplots(figsize=(grid_shape[1] * 2, grid_shape[0] * 2))

    # Get min/max Q values for consistent colorscale
    q_min, q_max = Q.min(), Q.max()

    # Action directions: [left, down, right, up]
    arrow_dx = [-0.35, 0, 0.35, 0]
    arrow_dy = [0, 0.35, 0, -0.35]

    # Create a grid background
    for i in range(grid_shape[0] + 1):
        ax.axhline(i, color="gray", linewidth=1)
    for j in range(grid_shape[1] + 1):
        ax.axvline(j, color="gray", linewidth=1)

    # Plot arrows for each state
    for state in range(Q.shape[0]):
        row = state // grid_shape[1]
        col = state % grid_shape[1]

        # Center of the cell
        cx, cy = col + 0.5, row + 0.5

        for action in range(4):
            q_value = Q[state, action]

            # Normalize color based on Q value
            if q_max > q_min:
                color_intensity = (q_value - q_min) / (q_max - q_min)
            else:
                color_intensity = 0.5

            # Color from red (low) to green (high)
            color = plt.cm.RdYlGn(color_intensity)  # type: ignore

            # Draw arrow
            ax.arrow(
                cx,
                cy,
                arrow_dx[action],
                arrow_dy[action],
                head_width=0.15,
                head_length=0.1,
                fc=color,
                ec="black",
                linewidth=1.5,
                alpha=0.8,
            )

            # Add text with Q value
            text_x = cx + arrow_dx[action] * 0.5
            text_y = cy + arrow_dy[action] * 0.7
            ax.text(
                text_x,
                text_y,
                f"{q_value:.2f}",
                ha="center",
                va="center",
                fontsize=8,
                weight="bold",
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.7),
            )

    # Set axis properties
    ax.set_xlim(0, grid_shape[1])
    ax.set_ylim(0, grid_shape[0])
    ax.set_aspect("equal")
    ax.invert_yaxis()
    ax.set_xticks(range(grid_shape[1] + 1))
    ax.set_yticks(range(grid_shape[0] + 1))
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.set_title("Q-Function Visualization", fontsize=14, weight="bold")

    # Add colorbar
    sm = plt.cm.ScalarMappable(
        cmap="RdYlGn", norm=plt.Normalize(vmin=q_min, vmax=q_max)  # type: ignore
    )
    sm.set_array([])
    plt.colorbar(sm, ax=ax, label="Q-Value")

    return ax
